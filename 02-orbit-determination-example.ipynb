{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orbit determination example\n",
    "This notebook does the following:\n",
    "* Download an orbit first guess from SpaceTrack\n",
    "* Download laser ranging data\n",
    "* Feed the data to Orekit\n",
    "* Estimate the orbit\n",
    "* Propagate and compare the orbit to the first guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of laser ranging data can be chosen (see below):\n",
    "\n",
    "* Normal point data: https://ilrs.cddis.eosdis.nasa.gov/data_and_products/data/npt/index.html\n",
    "* Full rate data: https://ilrs.cddis.eosdis.nasa.gov/data_and_products/data/frt/index.html\n",
    "    * This will improve the orbit estimation\n",
    "    * Caution, this format involves large quantities of data\n",
    "    * Caution 2, this data is unfiltered, therefore there can be a superposition of two range curves if two retro-reflectors on the satellite are visible by the station at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OD parameters\n",
    "First, some parameters need to be defined for the orbit determination:\n",
    "* Satellite ID in NORAD and COSPAR format\n",
    "* Spacecraft mass: important for the drag term\n",
    "* Measurement weights: used to weight certain measurements more than others during the orbit estimation. Here, we only have range measurements and we do not know the confidence associated to these measurements, so all weights are identical\n",
    "* OD date: date at which the orbit will be estimated. \n",
    "* Data collection duration: for example, if equals 2 days, the laser data from the 2 days before the OD date will be used to estimate the orbit. This value is an important trade-off for the quality of the orbit determination:\n",
    "    * The longer the duration, the more ranging data is available, which can increase the quality of the estimation\n",
    "    * The longer the duration, the longer the orbit must be propagated, and the higher the covariance because of the orbit perturbations such as the gravity field, drag, Sun, Moon, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "c = 299792458 # m/s\n",
    "\n",
    "# Parameters\n",
    "\n",
    "# Technosat\n",
    "#noradId = 42829\n",
    "#cosparId = '1704205' \n",
    "#sc_mass = 20.0 # kg\n",
    "#sc_crossSection = 0.10 # m2\n",
    "\n",
    "# Lageos-2\n",
    "noradId = 22195\n",
    "cosparId = '9207002' \n",
    "sc_mass = 405.0 # kg\n",
    "sc_crossSection = 0.2827 # m2\n",
    "\n",
    "cd_dragCoeff = 2.0 # TODO: proper value\n",
    "cr_radiationPressure = 1.0\n",
    "range_weight = 1.0 # Will be normalized later (i.e divided by the number of observations)\n",
    "range_sigma = 1.0 # Estimated covariance of the range measurements, in meters\n",
    "\n",
    "from datetime import datetime\n",
    "odDate = datetime(2018, 7, 14) # Beginning of the orbit determination\n",
    "collectionDuration = 1 # days\n",
    "from datetime import timedelta\n",
    "startCollectionDate = odDate + timedelta(days=-collectionDuration)\n",
    "\n",
    "# Orbit propagator parameters\n",
    "prop_min_step = 0.001 # s\n",
    "prop_max_step = 300.0 # s\n",
    "prop_position_error = 10.0 # m\n",
    "\n",
    "# Estimator parameters\n",
    "estimator_position_scale = 1.0 # m\n",
    "estimator_convergence_thres = 1e-3\n",
    "estimator_max_iterations = 25\n",
    "estimator_max_evaluations = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API credentials\n",
    "The following sets up accounts for SpaceTrack (for orbit data) and the EDC API (for laser ranging data).\n",
    "* A SpaceTrack account is required, it can be created for free at: https://www.space-track.org/auth/createAccount\n",
    "* An EDC account is required, it can be created for free at: https://edc.dgfi.tum.de/en/register/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter SpaceTrack password for account clement@jonglez.space ·····················\n",
      "Enter EDC API password for account jonglez ·······\n"
     ]
    }
   ],
   "source": [
    "# Space-Track\n",
    "identity_st = 'clement@jonglez.space' # Email address for Space-Track\n",
    "import getpass\n",
    "password_st = getpass.getpass(prompt='Enter SpaceTrack password for account {}'.format(identity_st))\n",
    "import spacetrack.operators as op\n",
    "from spacetrack import SpaceTrackClient\n",
    "st = SpaceTrackClient(identity=identity_st, password=password_st)\n",
    "\n",
    "# EDC API\n",
    "username_edc = 'jonglez' # Username for EDC API\n",
    "password_edc = getpass.getpass(prompt='Enter EDC API password for account {}'.format(username_edc)) # You will get prompted for your password\n",
    "url = 'https://edc.dgfi.tum.de/api/v1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up models\n",
    "Initializing Orekit and JVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orekit\n",
    "orekit.initVM()\n",
    "\n",
    "# Copy-pasted from https://gitlab.orekit.org/orekit-labs/python-wrapper/blob/master/python_files/pyhelpers.py\n",
    "from java.io import File\n",
    "from org.orekit.data import DataProvidersManager, DirectoryCrawler\n",
    "from orekit import JArray\n",
    "\n",
    "orekit_filename = 'orekit-data'\n",
    "DM = DataProvidersManager.getInstance()\n",
    "datafile = File(orekit_filename)\n",
    "if not datafile.exists():\n",
    "    print('Directory :', datafile.absolutePath, ' not found')\n",
    "\n",
    "crawler = DirectoryCrawler(datafile)\n",
    "DM.clearProviders()\n",
    "DM.addProvider(crawler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import station data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import slrDataUtils\n",
    "stationFile = 'SLRF2014_POS+VEL_2030.0_180504.snx'\n",
    "stationEccFile = 'ecc_xyz.snx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading station eccentricities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to convert column X to type <class 'numpy.float64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/laserod/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_cast_types\u001b[0;34m(self, values, cast_type, column)\u001b[0m\n\u001b[1;32m   1807\u001b[0m                 values = astype_nansafe(values, cast_type,\n\u001b[0;32m-> 1808\u001b[0;31m                                         copy=True, skipna=True)\n\u001b[0m\u001b[1;32m   1809\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/laserod/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;31m# Explicit copy, or required since NumPy can't view from / to object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '-61.9200-101.8670'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a187195686f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                  \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\s+|\\t+|\\s+\\t+|\\t+\\s+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                  \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Z'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                  skiprows=lineNumberStart, skipfooter=2)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/laserod/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/laserod/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/laserod/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nrows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/laserod/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m   2419\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_date_conversions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2421\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2422\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexnamerow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/laserod/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_convert_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2485\u001b[0m         return self._convert_to_ndarrays(data, clean_na_values,\n\u001b[1;32m   2486\u001b[0m                                          \u001b[0mclean_na_fvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2487\u001b[0;31m                                          clean_conv, clean_dtypes)\n\u001b[0m\u001b[1;32m   2488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_infer_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/laserod/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_convert_to_ndarrays\u001b[0;34m(self, dct, na_values, na_fvalues, verbose, converters, dtypes)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                         \u001b[0;31m# invalid input to is_bool_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                     \u001b[0mcvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cast_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/laserod/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_cast_types\u001b[0;34m(self, values, cast_type, column)\u001b[0m\n\u001b[1;32m   1809\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m                 raise ValueError(\"Unable to convert column %s to \"\n\u001b[0;32m-> 1811\u001b[0;31m                                  \"type %s\" % (column, cast_type))\n\u001b[0m\u001b[1;32m   1812\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to convert column X to type <class 'numpy.float64'>"
     ]
    }
   ],
   "source": [
    "#with open(stationEccFile) as f:\n",
    "#    line = ''\n",
    "#    for num, line in enumerate(f, 1):\n",
    "#        if '+SITE/ECCENTRICITY' in line:\n",
    "#            break\n",
    "#lineNumberStart = num + 1  # skipping table header\n",
    "#eccDataFrame = pd.read_csv(stationEccFile, engine='python',\n",
    "#                                 names=['SITE', 'PT', 'SOLN', 'T', 'DATA_START', 'DATA_END', 'XYZ', 'X', 'Y', 'Z', 'CDP-SOD'],\n",
    "#                                 index_col=['SITE', 'PT'],\n",
    "#                                 sep='\\s+|\\t+|\\s+\\t+|\\t+\\s+',\n",
    "#                                 dtype={'X':np.float64, 'Y':np.float64, 'Z':np.float64},\n",
    "#                                 skiprows=lineNumberStart, skipfooter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eccDataFrame['DATA_START'] = eccDataFrame['DATA_START'].apply(lambda x: epochStringToDatetime(x))\n",
    "#eccDataFrame['DATA_END'] = eccDataFrame['DATA_END'].apply(lambda x: epochStringToDatetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def str2float(leString):\n",
    "#    return float(leString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eccDataFrame['DATA_START'] = eccDataFrame['DATA_START'].apply(lambda x: epochStringToDatetime(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzokras/gitPerso/laser-orbit-determination/slrDataUtils.py:89: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  refEpoch = stationDataCsv.loc[indexTuple]['REF_EPOCH'][0]\n"
     ]
    }
   ],
   "source": [
    "stationData = slrDataUtils.parseStationData(stationFile, stationEccFile, startCollectionDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orbit determination needs a first guess. For this, we use Two-Line Elements. Retrieving the latest TLE prior to the beginning of the orbit determination. It is important to have a \"fresh\" TLE, because the newer the TLE, the better the orbit estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 22195U 92070B   18194.75154464 -.00000009 +00000-0 +00000-0 0  9996\n",
      "2 22195 052.6632 296.5716 0137876 001.6730 270.4693 06.47293829608184\n"
     ]
    }
   ],
   "source": [
    "rawTle = st.tle(norad_cat_id=noradId, epoch='<{}'.format(odDate), orderby='epoch desc', limit=1, format='tle')\n",
    "tleLine1 = rawTle.split('\\n')[0]\n",
    "tleLine2 = rawTle.split('\\n')[1]\n",
    "# Historical TLE in case Space-Track is down\n",
    "# tleLine1 = '1 42829U 17042E   18194.87572169 +.00000240 +00000-0 +29796-4 0  9990'\n",
    "# tleLine2 = '2 42829 097.5943 096.2213 0015726 108.7371 251.5556 14.90835887054309'\n",
    "print(tleLine1)\n",
    "print(tleLine2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Orekit frames and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.orekit.frames import FramesFactory\n",
    "from org.orekit.utils import IERSConventions\n",
    "tod = FramesFactory.getTOD(IERSConventions.IERS_2010, False) # Taking tidal effects into account when interpolating EOP parameters\n",
    "itrf = FramesFactory.getITRF(IERSConventions.IERS_2010, False)\n",
    "from org.orekit.models.earth import ReferenceEllipsoid\n",
    "wgs84Ellipsoid = ReferenceEllipsoid.getWgs84(itrf)\n",
    "from org.orekit.bodies import CelestialBodyFactory\n",
    "moon = CelestialBodyFactory.getMoon()\n",
    "sun = CelestialBodyFactory.getSun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the propagator from the initial TLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.orekit.propagation.analytical.tle import TLE\n",
    "orekitTle = TLE(tleLine1, tleLine2)\n",
    "\n",
    "from org.orekit.attitudes import NadirPointing\n",
    "nadirPointing = NadirPointing(tod, wgs84Ellipsoid)\n",
    "\n",
    "from org.orekit.propagation.analytical.tle import SGP4\n",
    "sgp4Propagator = SGP4(orekitTle, nadirPointing, sc_mass)\n",
    "\n",
    "tleInitialState = sgp4Propagator.getInitialState()\n",
    "tleEpoch = tleInitialState.getDate()\n",
    "tleOrbit_TEME = tleInitialState.getOrbit()\n",
    "tlePV_TOD = tleOrbit_TEME.getPVCoordinates(tod)\n",
    "\n",
    "from org.orekit.orbits import CartesianOrbit\n",
    "tleOrbit_TOD = CartesianOrbit(tlePV_TOD, tod, wgs84Ellipsoid.getGM())\n",
    "\n",
    "from org.orekit.propagation.conversion import DormandPrince853IntegratorBuilder\n",
    "integratorBuilder = DormandPrince853IntegratorBuilder(prop_min_step, prop_max_step, prop_position_error)\n",
    "\n",
    "from org.orekit.propagation.conversion import NumericalPropagatorBuilder\n",
    "from org.orekit.orbits import PositionAngle\n",
    "propagatorBuilder = NumericalPropagatorBuilder(tleOrbit_TOD,\n",
    "                                               integratorBuilder, PositionAngle.MEAN, estimator_position_scale)\n",
    "propagatorBuilder.setMass(sc_mass)\n",
    "propagatorBuilder.setAttitudeProvider(nadirPointing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding perturbation forces to the propagator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earth gravity field with degree 64 and order 64\n",
    "from org.orekit.forces.gravity.potential import GravityFieldFactory\n",
    "gravityProvider = GravityFieldFactory.getConstantNormalizedProvider(64, 64)\n",
    "from org.orekit.forces.gravity import HolmesFeatherstoneAttractionModel\n",
    "gravityAttractionModel = HolmesFeatherstoneAttractionModel(itrf, gravityProvider)\n",
    "propagatorBuilder.addForceModel(gravityAttractionModel)\n",
    "\n",
    "# Moon and Sun perturbations\n",
    "from org.orekit.forces.gravity import ThirdBodyAttraction\n",
    "moon_3dbodyattraction = ThirdBodyAttraction(moon)\n",
    "propagatorBuilder.addForceModel(moon_3dbodyattraction)\n",
    "sun_3dbodyattraction = ThirdBodyAttraction(sun)\n",
    "propagatorBuilder.addForceModel(sun_3dbodyattraction)\n",
    "\n",
    "# Solar radiation pressure\n",
    "from org.orekit.forces.radiation import IsotropicRadiationSingleCoefficient\n",
    "isotropicRadiationSingleCoeff = IsotropicRadiationSingleCoefficient(sc_crossSection, cr_radiationPressure);\n",
    "from org.orekit.forces.radiation import SolarRadiationPressure\n",
    "solarRadiationPressure = SolarRadiationPressure(sun, wgs84Ellipsoid.getEquatorialRadius(),\n",
    "                                                isotropicRadiationSingleCoeff)\n",
    "propagatorBuilder.addForceModel(solarRadiationPressure)\n",
    "\n",
    "# Atmospheric drag\n",
    "from org.orekit.forces.drag.atmosphere.data import MarshallSolarActivityFutureEstimation\n",
    "msafe = MarshallSolarActivityFutureEstimation(\n",
    "    '(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\p{Digit}\\p{Digit}\\p{Digit}\\p{Digit}F10\\.(?:txt|TXT)',\n",
    "    MarshallSolarActivityFutureEstimation.StrengthLevel.AVERAGE)\n",
    "DM.feed(msafe.getSupportedNames(), msafe) # Feeding the F10.7 bulletins to Orekit's data manager\n",
    "\n",
    "from org.orekit.forces.drag.atmosphere import NRLMSISE00\n",
    "atmosphere = NRLMSISE00(msafe, sun, wgs84Ellipsoid)\n",
    "#from org.orekit.forces.drag.atmosphere import DTM2000\n",
    "#atmosphere = DTM2000(msafe, sun, wgs84Ellipsoid)\n",
    "from org.orekit.forces.drag import IsotropicDrag\n",
    "isotropicDrag = IsotropicDrag(sc_crossSection, cd_dragCoeff)\n",
    "from org.orekit.forces.drag import DragForce\n",
    "dragForce = DragForce(atmosphere, isotropicDrag)\n",
    "propagatorBuilder.addForceModel(dragForce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.hipparchus.linear import QRDecomposer\n",
    "matrixDecomposer = QRDecomposer(1e-11)\n",
    "from org.hipparchus.optim.nonlinear.vector.leastsquares import GaussNewtonOptimizer\n",
    "optimizer = GaussNewtonOptimizer(matrixDecomposer, False)\n",
    "\n",
    "from org.orekit.estimation.leastsquares import BatchLSEstimator\n",
    "estimator = BatchLSEstimator(optimizer, propagatorBuilder)\n",
    "estimator.setParametersConvergenceThreshold(estimator_convergence_thres)\n",
    "estimator.setMaxIterations(estimator_max_iterations)\n",
    "estimator.setMaxEvaluations(estimator_max_evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching range data\n",
    "Looking for laser ranging data prior to the OD date.\n",
    "\n",
    "The API only allows to look for data using the date formats 2018-07-1%, 2018-07-14% or 2018-07-14 0% for example. As a consequence, the search must be split into several days. The results are then sorted, and the observations which are outside of the date range are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import slrDataUtils\n",
    "nptDatasetList = slrDataUtils.querySlrData(username_edc, password_edc, url, 'NPT',\n",
    "                                         cosparId, startCollectionDate, odDate)\n",
    "frdDatasetList = slrDataUtils.querySlrData(username_edc, password_edc, url, 'FRD',\n",
    "                                         cosparId, startCollectionDate, odDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the list of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orekit.pyhelpers import *\n",
    "from org.orekit.estimation.measurements import Range\n",
    "import slrDataUtils\n",
    "nptDataFrame = slrDataUtils.dlAndParseSlrData(username_edc, password_edc, url, 'NPT', nptDatasetList)\n",
    "# Comment out to download Full-Rate data\n",
    "# frdDataFrame = slrDataUtils.dlAndParseSlrData(username_edc, password_edc, url, 'FRD', frdDatasetList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the measurements to the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slrDataFrame = nptDataFrame\n",
    "# Comment out to use Full-Rate data\n",
    "# slrDataFrame = frdDataFrame\n",
    "for receiveTime, slrData in slrDataFrame.iterrows():\n",
    "    orekitRange = Range(stationData.loc[slrData['station-id'], 'OrekitGroundStation'], \n",
    "                        datetime_to_absolutedate(receiveTime),\n",
    "                        slrData['range'],\n",
    "                        range_sigma,\n",
    "                        range_weight\n",
    "                       ) # Uses date of signal reception; https://www.orekit.org/static/apidocs/org/orekit/estimation/measurements/Range.html\n",
    "    estimator.addMeasurement(orekitRange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the OD\n",
    "Estimate the orbit. This step can take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatedPropagatorArray = estimator.estimate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the estimated propagator and orbit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatedPropagator = estimatedPropagatorArray[0]\n",
    "estimatedInitialState = estimatedPropagator.getInitialState()\n",
    "actualOdDate = estimatedInitialState.getDate()\n",
    "estimatedOrbit_init = estimatedInitialState.getOrbit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance analysis\n",
    "Creating the LVLH frame, computing the covariance matrix in both TOD and LVLH frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the LVLH frame \n",
    "from org.orekit.frames import LocalOrbitalFrame\n",
    "from org.orekit.frames import LOFType\n",
    "lvlh = LocalOrbitalFrame(tod, LOFType.LVLH, estimatedPropagator, 'LVLH')\n",
    "\n",
    "# Getting covariance matrix in TOD frame\n",
    "covMat_tod_java = estimator.getPhysicalCovariances(1.0e-10)\n",
    "\n",
    "# Converting matrix to LVLH frame\n",
    "# Getting an inertial frame aligned with the LVLH frame at this instant\n",
    "# The LVLH is normally not inertial, but this should not affect results too much\n",
    "# Reference: David Vallado, Covariance Transformations for Satellite Flight Dynamics Operations, 2003\n",
    "tod2lvlh_frozen = tod.getTransformTo(lvlh, actualOdDate).freeze() \n",
    "\n",
    "# Computing Jacobian\n",
    "from org.orekit.utils import CartesianDerivativesFilter\n",
    "from orekit.pyhelpers import JArray_double2D\n",
    "jacobianDoubleArray = JArray_double2D(6, 6)\n",
    "tod2lvlh_frozen.getJacobian(CartesianDerivativesFilter.USE_PV, jacobianDoubleArray)\n",
    "from org.hipparchus.linear import Array2DRowRealMatrix\n",
    "jacobian = Array2DRowRealMatrix(jacobianDoubleArray)\n",
    "# Applying Jacobian to convert matrix to lvlh\n",
    "covMat_lvlh_java = jacobian.multiply(\n",
    "    covMat_tod_java.multiply(covMat_tod_java.transpose()))\n",
    "\n",
    "# Converting the Java matrices to numpy\n",
    "import numpy as np\n",
    "covarianceMat_tod = np.matrix([covMat_tod_java.getRow(iRow) \n",
    "                              for iRow in range(0, covMat_tod_java.getRowDimension())])\n",
    "covarianceMat_lvlh = np.matrix([covMat_lvlh_java.getRow(iRow) \n",
    "                              for iRow in range(0, covMat_lvlh_java.getRowDimension())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the position and velocity standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_std_crossTrack = np.sqrt(max(0.0, covarianceMat_lvlh[0,0]))\n",
    "pos_std_alongTrack = np.sqrt(max(0.0, covarianceMat_lvlh[1,1]))\n",
    "pos_std_outOfPlane = np.sqrt(max(0.0, covarianceMat_lvlh[2,2]))\n",
    "vel_std_crossTrack = np.sqrt(max(0.0, covarianceMat_lvlh[3,3])) # In case the value is negative...\n",
    "vel_std_alongTrack = np.sqrt(max(0.0, covarianceMat_lvlh[4,4]))\n",
    "vel_std_outOfPlane = np.sqrt(max(0.0, covarianceMat_lvlh[5,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing residuals\n",
    "Getting the estimated and measured ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propagatorParameters   = estimator.getPropagatorParametersDrivers(True)\n",
    "measurementsParameters = estimator.getMeasurementsParametersDrivers(True)\n",
    "\n",
    "lastEstimations = estimator.getLastEstimations()\n",
    "valueSet = lastEstimations.values()\n",
    "estimatedMeasurements = valueSet.toArray()\n",
    "keySet = lastEstimations.keySet()\n",
    "realMeasurements = keySet.toArray()\n",
    "\n",
    "from org.orekit.estimation.measurements import EstimatedMeasurement\n",
    "from org.orekit.estimation.measurements import Range\n",
    "\n",
    "import pandas as pd\n",
    "observedRangeSeries = pd.Series()\n",
    "estimatedRangeSeries = pd.Series()\n",
    "\n",
    "for estMeas in estimatedMeasurements:\n",
    "    estMeas = EstimatedMeasurement.cast_(estMeas)\n",
    "    observedValue = estMeas.getObservedValue()\n",
    "    estimatedValue = estMeas.getEstimatedValue()\n",
    "    pyDateTime = absolutedate_to_datetime(estMeas.date)\n",
    "    observedRangeSeries[pyDateTime] = observedValue[0]\n",
    "    estimatedRangeSeries[pyDateTime] = estimatedValue[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Plotly for offline mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x=observedRangeSeries.index, y=observedRangeSeries-estimatedRangeSeries,\n",
    "    mode='markers'\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Range residuals',\n",
    "    xaxis = dict(\n",
    "        title = 'Datetime UTC'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'Range residual (m)'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagating the solution \n",
    "Propagating the solution and saving the PV coordinates from both the estimated propagator and the initial TLE guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slrDataUtils import orekitPV2dataframe\n",
    "\n",
    "PV_tod_df = pd.DataFrame()\n",
    "PV_ecef_df = pd.DataFrame()\n",
    "PV_tle_tod_df = pd.DataFrame()\n",
    "deltaPV_lvlh_df = pd.DataFrame() # SGP4 PV from SLROD origin\n",
    "\n",
    "dt = 300.0\n",
    "date_start = datetime_to_absolutedate(startCollectionDate)\n",
    "date_end = datetime_to_absolutedate(odDate)\n",
    "date_current = date_start\n",
    "\n",
    "estimatedPropagator.resetInitialState(estimatedInitialState)\n",
    "while date_current.compareTo(date_end) <= 0:\n",
    "    datetime_current = absolutedate_to_datetime(date_current)    \n",
    "    spacecraftState = estimatedPropagator.propagate(date_current)\n",
    "    \n",
    "    PV_tod = spacecraftState.getPVCoordinates(tod)\n",
    "    PV_tod_df = PV_tod_df.append(orekitPV2dataframe(PV_tod, datetime_current))\n",
    "    \n",
    "    PV_ecef = spacecraftState.getPVCoordinates(itrf)\n",
    "    PV_ecef_df = PV_ecef_df.append(orekitPV2dataframe(PV_ecef, datetime_current))\n",
    "    \n",
    "    PV_tle_tod = sgp4Propagator.getPVCoordinates(date_current, tod)\n",
    "    PV_tle_tod_df = PV_tle_tod_df.append(orekitPV2dataframe(PV_tle_tod, datetime_current))\n",
    "    \n",
    "    deltaPV_lvlh = sgp4Propagator.getPVCoordinates(date_current, lvlh)\n",
    "    deltaPV_lvlh_df = deltaPV_lvlh_df.append(orekitPV2dataframe(deltaPV_lvlh, datetime_current))\n",
    "    \n",
    "    date_current = date_current.shiftedBy(dt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with TLE\n",
    "Plotting the position difference between the initial TLE and the estimated orbit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing norm of position difference\n",
    "import numpy as np\n",
    "deltaPos_norm = pd.Series(data=np.linalg.norm(\n",
    "    PV_tle_tod_df[['x','y','z']] - PV_tod_df[['x','y','z']], \n",
    "    axis=1),\n",
    "                         index=PV_tle_tod_df.index)\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x = deltaPos_norm.index,\n",
    "    y = deltaPos_norm,\n",
    "    mode='lines'\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Norm of position difference between TLE and estimation',\n",
    "    xaxis = dict(\n",
    "        title = 'Datetime UTC'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'Norm of position delta (m)'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the components of the position different between the TLE and the estimation, in LVLH frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "traceX = go.Scatter(\n",
    "    x = deltaPV_lvlh_df['x'].index,\n",
    "    y = deltaPV_lvlh_df['x'],\n",
    "    mode='lines',\n",
    "    name='Cross-Track'\n",
    ")\n",
    "\n",
    "traceY = go.Scatter(\n",
    "    x = deltaPV_lvlh_df['y'].index,\n",
    "    y = deltaPV_lvlh_df['y'],\n",
    "    mode='lines',\n",
    "    name='Along-Track'\n",
    ")\n",
    "\n",
    "traceZ = go.Scatter(\n",
    "    x = deltaPV_lvlh_df['z'].index,\n",
    "    y = deltaPV_lvlh_df['z'],\n",
    "    mode='lines',\n",
    "    name='Out-Of-Plane'\n",
    ")\n",
    "\n",
    "data = [traceX, traceY, traceZ]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Delta position between TLE and estimation in LVLH frame',\n",
    "    xaxis = dict(\n",
    "        title = 'Datetime UTC'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'Position difference (m)'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with CPF\n",
    "The EDC API also provides Consolidated Prediction Files, which contain spacecraft position/velocity in ITRF frame as generated by their orbit determination system. Let us compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requesting CPF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import slrDataUtils\n",
    "cpfList = slrDataUtils.queryCpfData(username_edc, password_edc, url, cosparId, startCollectionDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading and parsing CPF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpfDataFrame = slrDataUtils.dlAndParseCpfData(username_edc, password_edc, url, cpfList, startCollectionDate, odDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting position difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "traceX = go.Scatter(\n",
    "    x = cpfDataFrame['x'].index,\n",
    "    y = PV_ecef_df['x'] - cpfDataFrame['x'],\n",
    "    mode='lines',\n",
    "    name='X ECEF'\n",
    ")\n",
    "\n",
    "traceY = go.Scatter(\n",
    "    x = cpfDataFrame['y'].index,\n",
    "    y = PV_ecef_df['y'] - cpfDataFrame['y'],\n",
    "    mode='lines',\n",
    "    name='Y ECEF'\n",
    ")\n",
    "\n",
    "traceZ = go.Scatter(\n",
    "    x = cpfDataFrame['z'].index,\n",
    "    y = PV_ecef_df['z'] - cpfDataFrame['z'],\n",
    "    mode='lines',\n",
    "    name='Z ECEF'\n",
    ")\n",
    "\n",
    "data = [traceX, traceY, traceZ]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Delta position between CPF and estimation in ITRF frame',\n",
    "    xaxis = dict(\n",
    "        title = 'Datetime UTC'\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        title = 'Position difference (m)'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "trace = go.Scatter3d(\n",
    "    x = 1e-3 * PV_tod_df['x'],\n",
    "    y = 1e-3 * PV_tod_df['y'],\n",
    "    z = 1e-3 * PV_tod_df['z'],\n",
    "    mode='lines',\n",
    "    line=dict(\n",
    "        color='#1f77b4',\n",
    "        width=3\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = '3D trajectory plot in TOD frame',\n",
    "    scene=dict(\n",
    "        aspectmode='data'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
